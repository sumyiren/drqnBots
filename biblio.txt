20/7/2018

new project negotiation AI - the future marketplace?

export PATH=~/anaconda3/bin:$PATH


python train.py \
  --data data/negotiate \
  --cuda \
  --bsz 16 \
  --clip 0.5 \
  --decay_every 1 \
  --decay_rate 5.0 \
  --dropout 0.5 \
  --init_range 0.1 \
  --lr 1 \
  --max_epoch 30 \
  --min_lr 0.01 \
  --momentum 0.1 \
  --nembed_ctx 64 \
  --nembed_word 256 \
  --nesterov \
  --nhid_attn 256 \
  --nhid_ctx 64 \
  --nhid_lang 128 \
  --nhid_sel 256 \
  --nhid_strat 128 \
  --sel_weight 0.5 \
  --model_file sv_model.th

python reinforce.py \
  --data data/negotiate \
  --cuda \
  --bsz 16 \
  --clip 1 \
  --context_file data/negotiate/selfplay.txt \
  --eps 0.0 \
  --gamma 0.95 \
  --lr 0.5 \
  --momentum 0.1 \
  --nepoch 4 \
  --nesterov \
  --ref_text data/negotiate/train.txt \
  --rl_clip 1 \
  --rl_lr 0.2 \
  --score_threshold 6 \
  --sv_train_freq 4 \
  --temperature 0.5 \
  --alice_model sv_model.th \
  --bob_model sv_model.th \
  --output_model_file rl_model.th


python selfplay.py \
  --alice_model_file rl_model.th \
  --bob_model_file sv_model.th \
  --context_file data/negotiate/selfplay.txt \
  --temperature 0.5 \
  --log_file selfplay.log \
  --ref_text data/negotiate/train.txt





  wget https://repo.continuum.io/archive/Anaconda3-5.2.0-Linux-x86_64.sh


ssh yrs113@ae-jnr14-gpu.ae.ic.ac.uk


23/7/2018

so unfocussed...what the heck am i building?

the idea is a machine learnt negotiation system...which involves a seller and a buyer competing to get the best price possible for a certain product. 

something like an 'AI-negotiated market' for all types of goods. 

one condition is not being allowed to let buyers see the product...for example a car..needs a viewing...such things will affect negotiation but are unable to be inputed into the ai system for now.

gona look at the ebay industry...i.e ...ppl selling second hand goods, via postal (gumtree goods doesnt work).

the ai nego challenge, is now to convert it from a multi-issue bargaining task, into a buyer-seller price agreement scenario


24/7/2018

maybe a simple price-based offering instead of full on language communication

a reinforcement learner - adversarial

the seller constraints: lowest offering price, optimum offering price, deadline, number of potential buyers

the buyer constraints: asking price, lowest asking price, how much the buyer want item, 


quite excited by this....we start work in the morning.. a complete rewrite


25/7/2018

install 3.5 instead of 3.6 for anaconda to solve pandas core issue



26/7/2018

keep checking this: https://github.com/openai/baselines/issues/162

cant build separate seller buyer bots...instead gona train them togehter

and add and extract information as it goes

need to train independent bots? to ensure easier insertion later?


screw this moving to pytorch


27/7/2018

2-day break


1/8/2018

using maddpg to teach cooperation


2/8/2018

brunei to do list:
meet shing - done, amin - done, kb peeps - half, pei and frens - done, jh - done, kongtwins and zak - done, jia-done, dayat - done, edwin and gang, ying - done!
shahbandar - done
tungku beach - done
empire - done
movies - done
jolibee - done
labuan /miri?
kaizen - done
little audreys - done
dota - done
penyet - done
st andrews?
jiken rice - done

7/8/2018

https://github.com/MorvanZhou/pytorch-A3C/blob/master/discrete_A3C.py
back homeee\


13/8/2018

same robot for seller
multiple buyerbots


15/8/2018

argument is that multiple seller bots required as it begins with high value for each buyer bot

but how to make the seller bots communicate when they have leverage?


1/9/2018

got the cuda to work...back to new laptop

export PATH=/usr/local/cuda-9.2/bin${PATH:+:${PATH}}



11/9/2018

rocking chair test: what have you built in ur 20s


rethink reward structure



15/8/2018

super good explainer forward and backward:
https://medium.com/datathings/neural-networks-and-backpropagation-explained-in-a-simple-way-f540a3611f5e



16/8/2018

th.save and th.load so easy

have to redesign so that max number of sellers chosen....then randominity used..test this


19/9/2018

redesigned reward scheme again this morning



22/9/2018

sellerask buyerask doesnt converge...cant understand why


steam: r55283



23/9/2018

convergence acheived...however...have to redesign determination as it seems to screw with things



29/9/2018

been so long..back to work

rebuild determination

sometimes sellerbot or buyerbot takes over...competition disappears and one takes control


30/9/2018

buyer is very weak...keep giving in to the seller resulting in large negative returns


going to experiement with another version of maddpg-pytorch:
https://github.com/shariqiqbal2810/maddpg-pytorch/blob/master/algorithms/maddpg.py


dont manual random..depend on the sample noise randomness to fix it - for actor critic



03/10/2018

what i see is if the buyer is overpowered early (when nsellers >4), it becomes a stubborn addict, i.e
unwilling to compromise, detrimenting the maddpg sellers early

what to do what to do?



7/10/2018

create a reward structure that can produce compromise, competition and cooperation


right now saving the most competitive ones
need a balance to save both the compeititve and compromising ones

build cooperation tomoro



10/10/2018

building determination..more determination should result in higher vals?


14/10/2018

confused..determination is not pushing prices up as expected..why why why??


17/10/2018

logs:

3 nsellers, strongest copied over, 50-100, 20100 epi, 

remove copied over, build determination individually, determination = 1 very high for some reason


21/10/2018

changed to -2* for buyercase...does not work well, have to redefine system to charge higher determination more
2* 2**self.determination

real heroes of telemark


reward += 2**self.determination in both


no buyerincrementation rewards, seller incentivised to make deal, dqnbuyer beefed up


24/10/2018

model learn very slow
switch to ddqn


27/10/2018

need a complete rewrite...far too muddled right now..not shortcuts!

a3c might not work...requires parallelly hitting the environment

try using ddpg: https://github.com/ghliu/pytorch-ddpg


30/10/2018

should be some log function


gona try ddqn isntead...ddpg has failed me: https://flyyufelix.github.io/2017/10/12/dqn-vs-pg.html


had a look at office...at learning multiagent dqns...gona try that

https://arxiv.org/pdf/1605.06676.pdf

https://gist.github.com/spro/ef26915065225df65c1187562eca7ec4


1/11/2018

attempting quality of ppo
https://github.com/ikostrikov/pytorch-a2c-ppo-acktr - nvm


interesting: https://github.com/AndersonJo/dqn-pytorch/blob/master/dqn.py


2/11/2018

research at work today: 
maybe use a dqrn (since it requires several iterations for batching) for sellerbots
and use dqn for buyerbots

and then use dqrn for buyerbots trained off sellerbots

dqrn built off: https://github.com/Kyushik/GYM_DRL/blob/master/2_CartPole_DRQN.py
have to convert to pytorch


17/11/2018


ssh 2nd anser savior
https://askubuntu.com/questions/762541/ubuntu-16-04-ssh-sign-and-send-pubkey-signing-failed-agent-refused-operation



cuda https://askubuntu.com/questions/799184/how-can-i-install-cuda-on-ubuntu-16-04


export PATH=/usr/local/cuda-10.0/bin${PATH:+:${PATH}}


cuda bossss:
export PATH=${PATH}:/usr/local/cuda-9.0/bin
export CUDA_HOME=${CUDA_HOME}:/usr/local/cuda:/usr/local/cuda-9.0
export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/usr/local/cuda-9.0/lib64
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64

in ~/.profile




9/12/2018

export GOOGLE_APPLICATION_CREDENTIALS="/home/sumyiren/.ssh/NegoAI-5d051e17bbb4.json"

gcloud alpha cloud-shell ssh

source ~/.bashrc


13/12/2018
https://stackoverflow.com/questions/41607144/loading-two-models-from-saver-in-the-same-tensorflow-session

back to this question one seller many buyers - NG001


16/12/2018

NG002 implement save tomorrow


17/12/2018

gcloud ml-engine jobs submit training $JOB_NAME --module-name drqnMLEngine.task --job-dir $OUTPUT_PATH --package-path drqnMLEngine --region $REGION

JOB_NAME=test18
BUCKET_NAME=negoai204-mlengine
REGION=europe-west1
OUTPUT_PATH=gs://$BUCKET_NAME/output

NG 003 create test tester


20/12/2018
OUTPUT_PATH=./output
gcloud ml-engine local train --module-name drqnMLEngine.task --job-dir $OUTPUT_PATH --config config.yaml



31/12/2018

NG 004 auto tester and manual tester


JOB_NAME=test24_gpu
BUCKET_NAME=negoai204-mlengine
REGION=europe-west1
OUTPUT_PATH=gs://$BUCKET_NAME/$JOB_NAME/output


gcloud ml-engine jobs submit training $JOB_NAME --module-name drqnMLEngine.task --job-dir $OUTPUT_PATH --package-path drqnMLEngine --region $REGION --config config.yaml



1/1/2019

hny to me

trained model doesnt seem to exhibit the characteristics as during training...very weird

been on this problem the whole day....no progress


2/1/2019

download
gsutil cp -r $OUTPUT_PATH ./


[NG005] epsilon error, it was all random, also reduced step size to 25 (might change this back later on, this quickens training)

txqr so cool


3/1/2019

test15 is wrong - epsilon = 0


4/1/2019
https://stackoverflow.com/questions/51306862/how-to-use-tensorflow-gpu
https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html

a restart helps :)


epsilon fixed to 10% random i.e quite greedy


OUTPUT_PATH=./output/localtest42
python task.py --job-dir $OUTPUT_PATH


[NG006] radical solution, a randomed epsilon episode, not step



5/1/2019

[NG007] redesign the reward structure, - for the first case to make a deal, no matter how bad it is

deals being made, but sometimes seller over powers buyer and vice versa


6/1/2019

seller overpowered, seems to have first move advantage to pull it up, question is why localtest2

determination set to 0 for localtest3


7/1/2019

outbounds punished only localtest4


localtest5 more explorative 0.25 eps


8/1/2019

rethink the price structure [NG008]

             80                             100                             110
|------------|---------------|---------------|---------------|---------------|---------------|---------------|
          minPrice                      askingPrice                      maxPrice(buyer)



9/1/2019


[NG009] buyer should not his max price and seller should know its min price, remove determination for now

10/1/2019
localtest6 9/10 model

need to refactor to make maxPrice part of buyerproperties
train on two or more
write vc thing



jk not good enough, not boundaried

localtest7 alot of boundary conditions added, maxprice refactor [NG010]



13/1/2019

when there are two players, only the highest reward should be honored....right? for seller

[NG011] last step seller reward shared highest
nvm

OUTPUT=./output
python task.py --job-dir=$OUTPUT




try out dpiqn model for teamwork...stil unconvinced at the teamwork of drqn unforutnatley

https://github.com/williamd4112/infer-policy-features


14/1/2019

many seller method problem, cant tell which is the first or second state

but very scalable - can add parts

localtest8 - first two nSeller test - 3500 itrs

[NG011] last step seller reward shared highest - localtest9
i.e a no deal is possible
crediting bad behavior though??


Team spirit ranges from 0 to 1, putting a weight on how much each of OpenAI Five’s heroes should care about its individual reward function versus the average of the team’s reward functions. We anneal its value from 0 to 1 over training.


[NG012] Separating into multiple sellers (again)
- separate into a current seller first two
- remove repeating information from state sent
localtest10



17/1/2019

dont even have an idea name LOl


Idea Name: Tsukiji-AI

What problem are you solving:
When shopping online on Amazon, most goods are sold at a fixed price. Sellers and buyers are unable to negotiate on an optimum price for a product based on its demand. In an ideal negotiation scenario, high demand (many buyers) for a product would allow sellers to reach a higher price for its product, while low demand (few buyers) for a product would allow buyers to get a bargain for the product. One reasoning why this is not be done is because, nobody wants to stare at a computer screen for hours negotiating on these price points. Our solution is as follows.

What is your solution to this problem:
The solution involves training two different bots: sellerbots and buyerbots. A seller is given a sellerbot while buyers are each given a buyerbot. Sellerbots and buyerbots will then compete and negotiate in order to compromise on an optimum price. Based on parameters (i.e how much a buyer wants a product, how far a buyer is willing to go, how stingy a seller is), the bots are customized to suit the seller and buyer and will negotiate accordingly. What we envision is something like ebay, but which is powered by automatic negotiating bots.


Give us more detail what you have done:

When I say 'negotiating', I do not mean give the bots the ability to communicate via natural language (this is an interesting problem, but a bit out of scope). Instead we gave bots the ability to fluctuate the price they are willing to buy/sell.  Over set a time limit, these bots display an ability to 'fake' a high/low price, but slowly compromise towards an agreed price. 

These bots are trained adversarially using reinforcement learning. The whole training process is performed on cloud (Google ML-Engine, but I see that AWS is sponsoring, so definitely open to migrating to that :)). 

The challenge now is to train larger models, to build a viable business plan for this project, and to build a front-end to get user feedback.


19/1/2019

possible names: juke, runding, soyoro, barkeot, bonzuu, stock, name for the trader will be name of company

start with: M, P, negotiate, market, arke, reinforcement, adversary

broker

                  

20/1/2019

learn from experience, reinformcent learning, market seller buyer, observation, reward,

BYRL??   barket omera 

tsukiji-ai.com, seo nightmare :(


model is stable, I think, time to train larger model? 3nsellers?


21/1/2019

[NG013] make seller dimensions smaller by removing repetitions



22/1/2019
nseller = 3
test18gpu
3500
2 days and 39 mins

try training deeper models

[NG014] build to continue training from saved model - needs fixing
localtest11 done here - 4000

27/1/2019

fixed unsync error, such a glaring mistake


28/1/2019

[NG015] Adding zeros


29/1/2019

[NG016] anneal teamwork 0 to 1

localtest12 done here


1/2/2019

[NG017]
only 1 deal has to realistically be made by seller
localtest13


[NG018]
increasing lstm size for seller
localtest14


3/2/2019

[NG019] One seller 3 envs again

seller does not need 3 of the same type - repeating information - try this with float implementation for action
if not the other 2 drqn has no control over each other -difficult to communicate


[NG020] Only max used for seller when deal is made
localtest15


5/2/2019

did during cny half day
solved NG019
localtest16 - very explorative 0.2 eps


6/2/2019

localtest17 - using multiple rewards for sellers



9/2/2019

float system wonky, change back to integers, but 1 seller 3 buyers still



11/2/2019
PASS BY REFERENCE LULLLLLLL



17/2/2019

localtest19 started, with -300 reward



18/2/2019

sent to test21gpu - after +1 fix - tested to 2500

$ gcloud ml-engine jobs stream-logs test22_gpu



19/2/2019

sent to test22gpu - after 0 check (give -300 if still below 0)


20/2/2019

need a rethink, the Q-values are a wave - not discrete values - 3 of them should be independent


21/2/2019

Had a think at work today. Essentially, return to 3 independent sellers, but use teamSpirit to combine them. With regards to the timing issue, over a long time horizon, a small 1 step should not affect the long term thinking of the system. This is how it works for openai5 per say.

In order to do this, suggest one change [NG022] change from == to +1-1 for rewards to count as a success


localtest20 - annealed teamSpirit and random +1-1reward range 5050


24/2/2019

train katas while the thing is training - dont be lazy


reward structure with teamwork must change abit, must reference to constant plane [NG023] localtest21


4/3/2019

NG024 - testing quality of models!


6/3/2019

84/100 models reach deal!


7/3/2019

[NG025] - localtest24
buyers should be able to set their starting value and max value
sellers should be able to set their starting value and end value


             80              90                             100                             110
|------------|---------------|---------------|---------------|---------------|---------------|---------------|

     minPrice(seller)   startingprice(buyerask)     startingprice(sellerask)             maxPrice(buyer)


11/3/2019

server is being written, hopefully can deploy to heroku soon for user testing


20/3/2019

export YOURAPPLICATION_SETTINGS=/home/sumyiren/Documents/Development/NegoAICodebase/config.cfg


30/3/2019

what is my requirement?

sellerbot determines the minPrice and the startingPrice via randomised - but only shows the startingPrice to buyers

1 external player - playing against 2 other buyer bots, against a bot seller

external player determines the startingprice and maxprice - initial condition replicated for buyer bots

- first proof - bots can negotiate better than humans given the same intiial conditions


iron, money, pack, checkflights, checkpack, checkmap


check out: human negotiators rarely approach nash equilibrium, unless experienced 
actually monkeys do better than us :) -joeyyap 2019



11/4/2019

long month - swiss j ams mum - time to get back to work work


13/4/2019

redesign reward structure - ensure lower values for seller do not receive a beneficial reward


14/4/2019

first off, testing repeatability and teamwork

my actions do affect the others


17/4/2019
glaring mistake
[NG025] fix reward structure and train localtest25 



23/4/2019

need to punish seller more (as they benefit more from teamwork)



25/4/2019

the seller offers first, there buyer starting price should always be lower, raised to 200


27/4/2019

should be compared via min and maxprice, as these are the values that actually matter  - localtest28


29/4/2019

difference needs to be normalized by dividing by the min-max range - localtest29

30/4/2019

if negative reward, do not divide, help converge faster


2/5/2019

bad deal worse than no deal? 
bad deal = good deal for one bad for other

bad deal rejection should be rewarded with positive rewards
only negative rewards when theres a no deal on a possible deal



4/5/2019

add pressure to buyer - needs to know of other buyers in market, (use average price perhaps?)


6/5/2019

need a new revamp of the reward structure completely

how to add pressure to buyers - make them know there are other buyers in the market,

provide buyers with numbers of prices of other ppl - i.e [15 74 75 76 78 200]
                                                                 ^  ^  
                                                                b2  b3

reward structure also edited: for buyers - 0 if deal unmade, negative if bad deal made, positive if good deal, go back to simplification

[Test32] The pressure paradox


7/5/2019

90 200

first price auction?


16/5/2019

[Test33] bot1 vs bot 1 2 3, also, find a way to reduce possibilities


             80                             90                                              110
|------------|---------------|---------------|---------------|---------------|---------------|---------------|

   startingprice(buyerask)             minPrice(seller)                               maxPrice(buyer)
                                  startingprice(sellerask)




27/5/2019

go back to basics, 1 buyer, 2 buyer etc
[test34]


2/6/2019

[test34] potential reward shaping added


14/6/2019
[test35] 3 sellers

[test36] adding back:
             80              90                             100                             110
|------------|---------------|---------------|---------------|---------------|---------------|---------------|

     minPrice(seller)   startingprice(buyerask)     startingprice(sellerask)             maxPrice(buyer)


24/6/2019
test35 good?

[test37] shaping -10 for non following, revert back to test35 type (minPrice = startingPrice), 


22/7/2019

task is to expand testing module to give 3 different starting prices to buyers

27/7/2019
[Test39] Changing model to have two hidden layers


5/8/2019

problem we are facing, make the -0.5 * too large, and it wont make a deal to keep it low, make it -0.1* and it takes stupid deals

[Test41] Add negative 0.5 to no deal also


15/8/2019

Test 41 4500 okay model,


[Test43] Fixed small boo boo

[Test44] Remove hidden layer?